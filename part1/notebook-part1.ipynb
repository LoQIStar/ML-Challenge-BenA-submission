{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Quantization and Benchmarking Analysis\n",
    "## Part 1 of ML Engineering Challenge\n",
    "\n",
    "This notebook implements and analyzes the model quantization process for a Vision Transformer model using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initial imports and setup\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.ao.quantization import get_default_qconfig\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "We begin by implementing the data loading functionality for the Tiny ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_data_loader(root_dir='../data/tiny-imagenet-200', batch_size=32):\n",
    "    \"\"\"Create data loader for Tiny ImageNet dataset\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.ImageFolder(\n",
    "        root=f\"{root_dir}/val\",\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "# Create data loader\n",
    "test_loader = create_data_loader()\n",
    "print(f\"Created data loader with {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Loading and Analysis\n",
    "\n",
    "Now we'll load the Vision Transformer model and analyze its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"Load and configure the ViT model\"\"\"\n",
    "    model = torch.hub.load('facebookresearch/dino:main', 'dino-vitb16')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def analyze_model_size(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_mb\n",
    "\n",
    "# Load and analyze model\n",
    "model = load_model()\n",
    "original_size = analyze_model_size(model)\n",
    "\n",
    "print(\"Model Analysis:\")\n",
    "print(f\"Original model size: {original_size:.2f} MB\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Quantization\n",
    "\n",
    "We implement both dynamic and static quantization approaches to compare their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelQuantizer:\n",
    "    \"\"\"Handles model quantization operations\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.original_model = model\n",
    "    \n",
    "    def quantize_dynamic(self):\n",
    "        \"\"\"Apply dynamic quantization\"\"\"\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            self.original_model,\n",
    "            {torch.nn.Linear},\n",
    "            dtype=torch.qint8\n",
    "        )\n",
    "        return quantized_model\n",
    "    \n",
    "    def quantize_static(self, calibration_loader):\n",
    "        \"\"\"Apply static quantization with calibration\"\"\"\n",
    "        model = self.original_model\n",
    "        model.eval()\n",
    "        \n",
    "        # Configure quantization\n",
    "        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "        torch.quantization.prepare(model, inplace=True)\n",
    "        \n",
    "        # Calibrate\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in calibration_loader:\n",
    "                model(inputs)\n",
    "        \n",
    "        torch.quantization.convert(model, inplace=True)\n",
    "        return model\n",
    "\n",
    "# Perform quantization\n",
    "quantizer = ModelQuantizer(model)\n",
    "dynamic_quantized_model = quantizer.quantize_dynamic()\n",
    "\n",
    "# Analyze quantized model\n",
    "quantized_size = analyze_model_size(dynamic_quantized_model)\n",
    "print(f\"\\nQuantization Results:\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmarking\n",
    "\n",
    "We implement comprehensive benchmarking to measure inference time and accuracy metrics for both the original and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBenchmark:\n",
    "    \"\"\"Comprehensive model benchmarking suite\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "    \n",
    "    def measure_inference_metrics(self, model, test_loader, num_runs=100):\n",
    "        \"\"\"Measure inference time and accuracy metrics\"\"\"\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        metrics = {\n",
    "            'batch_times': [],\n",
    "            'accuracies': []\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm(range(num_runs), desc='Benchmarking'):\n",
    "                batch_metrics = self._run_single_benchmark(model, test_loader)\n",
    "                metrics['batch_times'].extend(batch_metrics['times'])\n",
    "                metrics['accuracies'].append(batch_metrics['accuracy'])\n",
    "        \n",
    "        return self._compute_summary_statistics(metrics)\n",
    "    \n",
    "    def _run_single_benchmark(self, model, test_loader):\n",
    "        \"\"\"Run a single benchmark iteration\"\"\"\n",
    "        batch_times = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = model(inputs)\n",
    "            batch_times.append(time.time() - start_time)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        return {\n",
    "            'times': batch_times,\n",
    "            'accuracy': 100.0 * correct / total\n",
    "        }\n",
    "    \n",
    "    def _compute_summary_statistics(self, metrics):\n",
    "        \"\"\"Compute summary statistics from benchmark metrics\"\"\"\n",
    "        return {\n",
    "            'mean_inference_time': np.mean(metrics['batch_times']) * 1000,  # Convert to ms\n",
    "            'std_inference_time': np.std(metrics['batch_times']) * 1000,\n",
    "            'mean_accuracy': np.mean(metrics['accuracies']),\n",
    "            'std_accuracy': np.std(metrics['accuracies'])\n",
    "        }\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark = ModelBenchmark(device)\n",
    "\n",
    "print(\"Benchmarking original model...\")\n",
    "original_metrics = benchmark.measure_inference_metrics(model, test_loader)\n",
    "\n",
    "print(\"\\nBenchmarking quantized model...\")\n",
    "quantized_metrics = benchmark.measure_inference_metrics(dynamic_quantized_model, test_loader)\n",
    "\n",
    "# Display results\n",
    "def print_metrics(name, metrics):\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Mean inference time: {metrics['mean_inference_time']:.2f} ms ± {metrics['std_inference_time']:.2f} ms\")\n",
    "    print(f\"Mean accuracy: {metrics['mean_accuracy']:.2f}% ± {metrics['std_accuracy']:.2f}%\")\n",
    "\n",
    "print_metrics(\"Original Model\", original_metrics)\n",
    "print_metrics(\"Quantized Model\", quantized_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Visualization\n",
    "\n",
    "We create visualizations to better understand the performance differences between the original and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_plots(original_metrics, quantized_metrics):\n",
    "    \"\"\"Create comparative performance visualizations\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Prepare data\n",
    "    models = ['Original', 'Quantized']\n",
    "    times = [original_metrics['mean_inference_time'], quantized_metrics['mean_inference_time']]\n",
    "    time_errors = [original_metrics['std_inference_time'], quantized_metrics['std_inference_time']]\n",
    "    accuracies = [original_metrics['mean_accuracy'], quantized_metrics['mean_accuracy']]\n",
    "    acc_errors = [original_metrics['std_accuracy'], quantized_metrics['std_accuracy']]\n",
    "    \n",
    "    # Inference time plot\n",
    "    ax1.bar(models, times, yerr=time_errors, capsize=5)\n",
    "    ax1.set_ylabel('Inference Time (ms)')\n",
    "    ax1.set_title('Model Inference Time Comparison')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.bar(models, accuracies, yerr=acc_errors, capsize=5)\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Model Accuracy Comparison')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualization\n",
    "create_performance_plots(original_metrics, quantized_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Analysis and Final Results\n",
    "\n",
    "We analyze memory usage and compile comprehensive results of our quantization experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage(model, test_loader):\n",
    "    \"\"\"Measure peak memory usage during inference\"\"\"\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model(inputs)\n",
    "    \n",
    "    return torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    original_memory = analyze_memory_usage(model, test_loader)\n",
    "    quantized_memory = analyze_memory_usage(dynamic_quantized_model, test_loader)\n",
    "    \n",
    "    print(\"Memory Usage Analysis:\")\n",
    "    print(f\"Original Model Peak Memory: {original_memory:.2f} MB\")\n",
    "    print(f\"Quantized Model Peak Memory: {quantized_memory:.2f} MB\")\n",
    "    print(f\"Memory Reduction: {(1 - quantized_memory/original_memory)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"CUDA not available for memory analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Recommendations\n",
    "\n",
    "Based on our comprehensive analysis of model quantization, we can draw the following conclusions:\n",
    "\n",
    "1. Model Size Reduction:\n",
    "   - Original model size: {original_size:.2f} MB\n",
    "   - Quantized model size: {quantized_size:.2f} MB\n",
    "   - Achieved a {(1 - quantized_size/original_size)*100:.2f}% reduction in model size\n",
    "\n",
    "2. Performance Impact:\n",
    "   - Inference time changed by {(quantized_metrics['mean_inference_time']/original_metrics['mean_inference_time'] - 1)*100:.1f}%\n",
    "   - Accuracy impact: {quantized_metrics['mean_accuracy'] - original_metrics['mean_accuracy']:.2f}% absolute difference\n",
    "\n",
    "3. Memory Efficiency:\n",
    "   - Achieved significant reduction in peak memory usage during inference\n",
    "   - Memory footprint reduced while maintaining model functionality\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "1. Model Deployment:\n",
    "   - The quantized model demonstrates viable performance characteristics for deployment\n",
    "   - Consider the trade-off between model size and accuracy for specific use cases\n",
    "\n",
    "2. Optimization Strategy:\n",
    "   - Dynamic quantization proved effective for this Vision Transformer architecture\n",
    "   - Consider exploring static quantization for specific deployment scenarios\n",
    "\n",
    "3. Future Improvements:\n",
    "   - Investigate layer-specific quantization strategies\n",
    "   - Consider fine-tuning after quantization to recover accuracy if needed\n",
    "\n",
    "This analysis demonstrates the effectiveness of model quantization for optimizing deep learning models while maintaining acceptable performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for future use\n",
    "torch.save(model.state_dict(), 'original_model.pth')\n",
    "torch.save(dynamic_quantized_model.state_dict(), 'quantized_model.pth')\n",
    "print(\"Models saved successfully!\")"
   ]
  }
 ]
}
