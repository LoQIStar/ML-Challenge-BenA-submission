{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conversion to TensorRT and ONNX\n",
    "\n",
    "This notebook implements the conversion of our trained model to ONNX and TensorRT formats for optimized inference on NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_path': '../part2/outputs/best_model.pth',\n",
    "    'output_dir': Path('./outputs'),\n",
    "    'batch_size': 32,\n",
    "    'image_size': 64,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "CONFIG['output_dir'].mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONNXExporter:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.model = self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        model = CNN(checkpoint['config'])  # CNN class from Part 2\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        return model\n",
    "        \n",
    "    def export(self, output_path):\n",
    "        dummy_input = torch.randn(1, 3, CONFIG['image_size'], CONFIG['image_size'])\n",
    "        \n",
    "        torch.onnx.export(\n",
    "            self.model,\n",
    "            dummy_input,\n",
    "            output_path,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={\n",
    "                'input': {0: 'batch_size'},\n",
    "                'output': {0: 'batch_size'}\n",
    "            },\n",
    "            opset_version=13\n",
    "        )\n",
    "        \n",
    "        # Verify ONNX model\n",
    "        onnx_model = onnx.load(output_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        \n",
    "        logging.info(f\"ONNX model exported and verified: {output_path}\")\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorRT Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorRTConverter:\n",
    "    def __init__(self):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        \n",
    "    def build_engine(self, onnx_path, engine_path):\n",
    "        builder = trt.Builder(self.logger)\n",
    "        network = builder.create_network(\n",
    "            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "        )\n",
    "        parser = trt.OnnxParser(network, self.logger)\n",
    "        \n",
    "        # Parse ONNX\n",
    "        with open(onnx_path, 'rb') as model:\n",
    "            parser.parse(model.read())\n",
    "        \n",
    "        config = builder.create_builder_config()\n",
    "        config.max_workspace_size = 1 << 30  # 1GB\n",
    "        \n",
    "        # Enable FP16 if possible\n",
    "        if builder.platform_has_fast_fp16:\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "        \n",
    "        # Set optimization profile\n",
    "        profile = builder.create_optimization_profile()\n",
    "        profile.set_shape(\n",
    "            'input',\n",
    "            (1, 3, CONFIG['image_size'], CONFIG['image_size']),  # min\n",
    "            (CONFIG['batch_size'], 3, CONFIG['image_size'], CONFIG['image_size']),  # opt\n",
    "            (CONFIG['batch_size']*2, 3, CONFIG['image_size'], CONFIG['image_size'])  # max\n",
    "        )\n",
    "        config.add_optimization_profile(profile)\n",
    "        \n",
    "        # Build and save engine\n",
    "        engine = builder.build_engine(network, config)\n",
    "        with open(engine_path, 'wb') as f:\n",
    "            f.write(engine.serialize())\n",
    "            \n",
    "        logging.info(f\"TensorRT engine built and saved: {engine_path}\")\n",
    "        return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceBenchmark:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def benchmark_pytorch(self, model, input_shape, n_iterations=100):\n",
    "        model = model.to(CONFIG['device'])\n",
    "        dummy_input = torch.randn(input_shape).to(CONFIG['device'])\n",
    "        \n",
    "        # Warmup\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_iterations):\n",
    "                start = torch.cuda.Event(enable_timing=True)\n",
    "                end = torch.cuda.Event(enable_timing=True)\n",
    "                \n",
    "                start.record()\n",
    "                _ = model(dummy_input)\n",
    "                end.record()\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                times.append(start.elapsed_time(end))\n",
    "        \n",
    "        self.results['pytorch'] = {\n",
    "            'mean': np.mean(times),\n",
    "            'std': np.std(times)\n",
    "        }\n",
    "        return self.results['pytorch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Export to ONNX\n",
    "    logging.info(\"Exporting model to ONNX...\")\n",
    "    exporter = ONNXExporter(CONFIG['model_path'])\n",
    "    onnx_path = CONFIG['output_dir'] / 'model.onnx'\n",
    "    exporter.export(onnx_path)\n",
    "    \n",
    "    # Convert to TensorRT\n",
    "    logging.info(\"Converting to TensorRT...\")\n",
    "    converter = TensorRTConverter()\n",
    "    engine_path = CONFIG['output_dir'] / 'model.engine'\n",
    "    engine = converter.build_engine(onnx_path, engine_path)\n",
    "    \n",
    "    # Benchmark\n",
    "    logging.info(\"Running inference benchmarks...\")\n",
    "    benchmark = InferenceBenchmark()\n",
    "    \n",
    "    # PyTorch benchmark\n",
    "    pytorch_results = benchmark.benchmark_pytorch(\n",
    "        exporter.model,\n",
    "        (CONFIG['batch_size'], 3, CONFIG['image_size'], CONFIG['image_size'])\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    logging.info(\"\\nBenchmark Results:\")\n",
    "    logging.info(f\"PyTorch - Mean: {pytorch_results['mean']:.2f}ms Â± {pytorch_results['std']:.2f}ms\")\n",
    "    \n",
    "    return engine_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
