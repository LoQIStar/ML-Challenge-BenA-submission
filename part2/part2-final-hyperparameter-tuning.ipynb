{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for CNN on Tiny ImageNet\n",
    "\n",
    "This notebook implements an automated hyperparameter optimization pipeline for CNN architecture using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_dir': Path('../data/tiny-imagenet-200'),\n",
    "    'output_dir': Path('./outputs'),\n",
    "    'num_classes': 200,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "# Setup logging and directories\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "CONFIG['output_dir'].mkdir(exist_ok=True)\n",
    "print(f\"Using device: {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = self._get_transforms()\n",
    "\n",
    "    def _get_transforms(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                               [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def get_loaders(self, batch_size):\n",
    "        train_dataset = ImageFolder(\n",
    "            self.data_dir / 'train',\n",
    "            self.transform\n",
    "        )\n",
    "        val_dataset = ImageFolder(\n",
    "            self.data_dir / 'val',\n",
    "            self.transform\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.features = self._make_features(config)\n",
    "        self.classifier = self._make_classifier(config)\n",
    "\n",
    "    def _make_features(self, config):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "\n",
    "        for i in range(config['num_conv_layers']):\n",
    "            out_channels = config['channels'][i]\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2)\n",
    "            ])\n",
    "            in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_classifier(self, config):\n",
    "        feature_size = 64 // (2 ** config['num_conv_layers'])\n",
    "        flatten_size = config['channels'][-1] * feature_size * feature_size\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flatten_size, config['hidden_size']),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(config['hidden_size'], CONFIG['num_classes'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion):\n",
    "        self.model = model.to(CONFIG['device'])\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.best_accuracy = 0\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            targets = targets.to(CONFIG['device'])\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(CONFIG['device'])\n",
    "                targets = targets.to(CONFIG['device'])\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "\n",
    "        return running_loss / len(val_loader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    def __init__(self):\n",
    "        self.study = optuna.create_study(direction=\"maximize\")\n",
    "        self.data_manager = DataManager(CONFIG['data_dir'])\n",
    "\n",
    "    def suggest_params(self, trial):\n",
    "        return {\n",
    "            'num_conv_layers': trial.suggest_int('num_conv_layers', 3, 5),\n",
    "            'channels': [trial.suggest_int(f'channels_{i}', 32, 256, 32) \n",
    "                        for i in range(5)],\n",
    "            'hidden_size': trial.suggest_int('hidden_size', 512, 2048, 128),\n",
    "            'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "            'learning_rate': trial.suggest_float('lr', 1e-5, 1e-2, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "        }\n",
    "\n",
    "    def objective(self, trial):\n",
    "        config = self.suggest_params(trial)\n",
    "        train_loader, val_loader = self.data_manager.get_loaders(config['batch_size'])\n",
    "\n",
    "        model = CNN(config)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        trainer = Trainer(model, optimizer, criterion)\n",
    "\n",
    "        for epoch in range(10):\n",
    "            train_loss, train_acc = trainer.train_epoch(train_loader)\n",
    "            val_loss, val_acc = trainer.validate(val_loader)\n",
    "            \n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return trainer.best_accuracy\n",
    "\n",
    "    def optimize(self, n_trials=100):\n",
    "        self.study.optimize(self.objective, n_trials=n_trials)\n",
    "        return self.study.best_params, self.study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(config, epochs=30):\n",
    "    data_manager = DataManager(CONFIG['data_dir'])\n",
    "    train_loader, val_loader = data_manager.get_loaders(config['batch_size'])\n",
    "    \n",
    "    model = CNN(config)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model, optimizer, criterion)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = trainer.train_epoch(train_loader)\n",
    "        val_loss, val_acc = trainer.validate(val_loader)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config,\n",
    "                'accuracy': val_acc\n",
    "            }, CONFIG['output_dir'] / 'best_model.pth')\n",
    "        \n",
    "        logging.info(f'Epoch {epoch+1}/{epochs} - '\n",
    "                    f'Train Acc: {train_acc:.2f}%, '\n",
    "                    f'Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Run hyperparameter optimization\n",
    "    optimizer = HyperparameterOptimizer()\n",
    "    best_params, best_value = optimizer.optimize(n_trials=100)\n",
    "    \n",
    "    logging.info(f\"Best validation accuracy: {best_value:.2f}%\")\n",
    "    logging.info(\"Best hyperparameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        logging.info(f\"{param}: {value}\")\n",
    "    \n",
    "    # Train final model\n",
    "    final_accuracy = train_final_model(best_params)\n",
    "    logging.info(f\"Final model accuracy: {final_accuracy:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'best_accuracy': final_accuracy\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ]
}